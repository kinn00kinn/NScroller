## 🔧 バックエンド要件定義書 Ver. 1.0

### 1. 概要

本ドキュメントは、「Niche Scroller (AI Edition)」のバックエンド機能に関する要件を定義する。
バックエンドは、フロントエンド（Next.js on Vercel）およびデータベース（Supabase）と連携し、以下の2つの主要な責務を担う。

1.  **データ収集バッチ:** AI関連の外部情報源から定期的に記事データを収集し、データベースに格納する。
2.  **データ提供API:** フロントエンドの無限スクロール機能に対し、ページ分割された記事データを提供する。

### 2. データ収集バッチ

  * **目的:** `全体要件定義.md` で定められた情報源から、新規記事を自動で収集・蓄積する。
  * **実行環境:** GitHub Actions のスケジュール実行（`cron`）を利用する。
  * **実行頻度:** 1時間に1回。
  * **実装言語:** Python または Node.js/TypeScript。

#### 2.1. 処理フロー

1.  **情報源の読み込み:** `全体要件定義.md` の「4.2. 情報源 (RSSフィード) 一覧」に記載された全RSSフィードURLのリストを取得する。
2.  **RSSフィードの巡回:** 各URLに対してHTTPリクエストを送信し、RSSフィード（XML）を取得する。
3.  **記事のパース:** 取得したXMLをパースし、個々の記事情報（タイトル、URL、公開日時など）を抽出する。
    *   OGP画像（`image_url`）の取得も試みる。RSSフィード内にOGP画像URLが含まれていない場合、記事本文のHTMLを取得して `og:image` タグから抽出する処理を検討する（Should要件）。
4.  **重複チェック:** 記事の `article_url` をキーとして、データベース（Supabaseの `articles` テーブル）に同一URLの記事が既に存在するかを確認する。
5.  **データ保存:** 重複がない新規記事のみ、`全体要件定義.md` の「4.1. 収集対象データ項目」で定義されたスキーマに従って、`articles` テーブルにレコードを挿入する。
    *   `published_at` はタイムゾーン情報を含めて正しく `TIMESTAMP` 型で保存する。
    *   `created_at` には現在のタイムスタンプを記録する。

### 3. データ提供API

  * **目的:** フロントエンドの無限スクロール（2ページ目以降のデータ読み込み）を実現するためのAPIエンドポイントを提供する。
  * **実装方式:** Next.js の **Route Handler** を利用する。
  * **エンドポイント:** `GET /api/posts`
  * **ホスティング:** Vercel (Serverless Functions)

#### 3.1. リクエスト仕様

  * **クエリパラメータ:**
    *   `page` (number, optional, default: `1`): 取得したいデータのページ番号。
    *   `limit` (number, optional, default: `20`): 1ページあたりの記事件数。

#### 3.2. レスポンス仕様

  * **成功時 (200 OK):**
    *   **形式:** JSON
    *   **ボディ (例):**
      ```json
      {
        "articles": [
          {
            "id": "...",
            "title": "...",
            "article_url": "...",
            "published_at": "...", // ISO 8601 形式
            "source_name": "...",
            "image_url": "..."
          }
        ]
      }
      ```
  * **エラー時:**
    *   `500 Internal Server Error`: データベース接続エラーなど、サーバー内部で問題が発生した場合。

#### 3.3. 処理詳細

1.  リクエストから `page` と `limit` のクエリパラメータを取得する。
2.  `offset` を計算する (例: `(page - 1) * limit`)。
3.  Supabaseクライアントを使用し、`articles` テーブルから以下の条件でデータを取得する。
    *   `ORDER BY published_at DESC` (公開日の新しい順)
    *   `LIMIT limit`
    *   `OFFSET offset`
4.  取得したデータをJSON形式でレスポンスとして返す。

### 4. データベース (Supabase)

  * **テーブル名:** `articles`
  * **スキーマ:** `全体要件定義.md` の「4.1. 収集対象データ項目」に準拠する。
    *   `article_url` カラムには **UNIQUE制約** を設定し、データベースレベルで記事の重複を防止する。
